---
title: "Count Outcomes"
output: html_document
---

---
title       : Count outcomes, Poisson GLMs
subtitle    : Regression Models
author      : Brian Caffo, Jeffrey Leek, Roger Peng 
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../librariesNew
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
require("knitr")
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig_03_03/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
setwd("~/git_folder/datascience/regmod/regression_models_notes/")
```

## Key ideas

* Many data take the form of counts
  * Calls to a call center
  * Number of flu cases in an area
  * Number of cars that cross a bridge
* Data may also be in the form of rates
  * Percent of children passing a test
  * Percent of hits to a website from a country
* Linear regression with transformation is an option

---

## Poisson distribution
- The Poisson distribution is a useful model for counts and rates
- Here a rate is count per some monitoring time
- Some examples uses of the Poisson distribution
    - Modeling web traffic hits
    - Incidence rates
    - Approximating binomial probabilities with small $p$ and large $n$
    - Analyzing contigency table data

---

## The Poisson mass function
- $X \sim Poisson(t\lambda)$ if
$$
P(X = x) = \frac{(t\lambda)^x e^{-t\lambda}}{x!}
$$
For $x = 0, 1, \ldots$.
- The mean of the Poisson is $E[X] = t\lambda$, thus $E[X / t] = \lambda$
- The variance of the Poisson is $Var(X) = t\lambda$.
- The Poisson tends to a normal as $t\lambda$ gets large.

---

```{r simPois,fig.height=4,fig.width=8, cache=TRUE}
par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = "h", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = "h", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = "h", frame = FALSE) 
```

---

## Poisson distribution
### Sort of, showing that the mean and variance are equal
```{r}
x <- 0 : 10000; lambda = 3
mu <- sum(x * dpois(x, lambda = lambda))
sigmasq <- sum((x - mu)^2 * dpois(x, lambda = lambda))
c(mu, sigmasq)
```

---

## Example: Leek Group Website Traffic
* Consider the daily counts to Jeff Leek's web site

[http://biostat.jhsph.edu/~jleek/](http://biostat.jhsph.edu/~jleek/)

* Since the unit of time is always one day, set $t = 1$ and then
the Poisson mean is interpretted as web hits per day. (If we set $t = 24$, it would
be web hits per hour).

---

## Website data

```{r leekGet,echo=FALSE,eval=FALSE}
dir.create(path = paste0(getwd(),"/files_03_03"))
download.file("https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda",destfile=paste0(getwd(),"/files_03_03/gaData.rda"),method="curl")
```

```{r leekLoad,cache=TRUE}
load("./files_03_03/gaData.rda")
gaData$julian <- julian(gaData$date)
head(gaData)
```

[http://skardhamar.github.com/rga/](http://skardhamar.github.com/rga/)


---

## Plot data

```{r, dependson="leekLoad",fig.height=4.5,fig.width=4.5}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
```


---

## Linear regression

$$ NH_i = b_0 + b_1 JD_i + e_i $$

$NH_i$ - number of hits to the website

$JD_i$ - day of the year (Julian day)

$b_0$ - number of hits on Julian day 0 (1970-01-01)

$b_1$ - increase in number of hits per unit day

$e_i$ - variation due to everything we didn't measure


---

## Linear regression line

```{r linReg, dependson="leekLoad",fig.height=4,fig.width=4, cache=TRUE}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col="red",lwd=3)
```
---

## Aside, taking the log of the outcome
- Taking the natural log of the outcome has a specific interpretation.
- Consider the model

$$ \log(NH_i) = b_0 + b_1 JD_i + e_i $$

$NH_i$ - number of hits to the website

$JD_i$ - day of the year (Julian day)

$b_0$ - log number of hits on Julian day 0 (1970-01-01)

$b_1$ - increase in log number of hits per unit day

$e_i$ - variation due to everything we didn't measure

* This is not the Poisson model,
    * which would model the conceptual mean of Y, $\log(\mu_i)$, 
    * which would solve the main problem here.
* The log of 0 is undefined, so taking the log of outcomes which are 0 is problematic.
* Logging outcomes imparts a particular interpretation of the coefficients when you exponentiate them. 

---

## Exponentiating coefficients
- $e^{E[\log(Y)]}$ geometric mean of $Y$. 
    - y must be positive.
    - With no covariates, this is estimated by $e^{\frac{1}{n}\sum_{i=1}^n \log(y_i)} = (\prod_{i=1}^n y_i)^{1/n}$
    - That's the geometric mean.
    - Useful for relative changes, i.e. 1.03%
- When you take the natural log of outcomes and fit a regression model, your exponentiated coefficients
estimate things about geometric means, rather than about the natural scale means.
- $e^{\beta_0}$ estimated geometric mean hits on day 0
- $e^{\beta_1}$ estimated relative increase or decrease in geometric mean hits per day
- There's a problem with logs when you have zero counts (they're undefined).
    - Adding a constant works (`+ 1` or `+ 0.5`, etc.).
- The problem with adding a constant is that it's less inuitive to read.
    - The interpretation is now, for example, the relative change in geometric mean hits **+ 1** per day.
```{r}
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
```
Here we see a 0.2% increase in hits per day.

---

## Linear vs. Poisson regression

__Linear__

The Linear Model models the number of hits as a linear relationship:
$$ NH_i = b_0 + b_1 JD_i + e_i $$

which implies an expected value relationship between the outcome and the linear predictor:
$$ E[NH_i | JD_i, b_0, b_1] = b_0 + b_1 JD_i$$

__Poisson/log-linear__

The Poisson Log-linear Model models the log of the expected value as the linear predictor:  
$$ \log\left(E[NH_i | JD_i, b_0, b_1]\right) = b_0 + b_1 JD_i $$

which we can invert to show that the expected value is the exponent of the linear predictor:
$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$

- Notice how might interpret $b_1$ here:
The expected value for the number of hits for a Julian date that equals j+1 minus that which equals j, equals beta_1.
$$E[NH_i | JD_i = j+1] - E[NH_i | JD_i = j] = b_1$$
- We can raise both sides by $e$, and that is the relative increase in the mean number of hits per one-day increase.
    - If there were other covariates, we would consider them to be held constant.

Modeling the Log Counts as opposed to the Poisson Log-Linear:
- The Log Counts Model models the expected value of the log of the number of hits, i.e. the mean of the log of the outcomes directly.
    $$E[\log(NH_i)]$$
- Differs from the Poisson Model which logs the expected value, i.e. the log of the mean.

---

## Multiplicative differences

<br><br>
$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$

<br><br>

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 \right)\exp\left(b_1 JD_i\right) $$

<br><br>

If $JD_i$ is increased by one unit, $E[NH_i | JD_i, b_0, b_1]$ is multiplied by $\exp\left(b_1\right)$

So $e^{\beta_1}$ is the relative change in expected web traffic per 1-day increase.

---

## Poisson regression in R

```{r poisReg, dependson="linReg",fig.height=4.5,fig.width=4.5, cache=TRUE}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")

## Linear Model fit
abline(lm1,col="red",lwd=3)

## GLM fit
lines(gaData$julian,glm1$fitted,col="blue",lwd=3)
```
Notice that the glm fit (blue) is curved because it's on a log of the mean scale.

---

## Mean-variance relationship?

```{r, dependson="poisReg",fig.height=4.5,fig.width=4.5}
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")
```

Instead of `family = poisson` in the glm model, we can use `family = quasipoisson`, which uses a multiplicative factor with the mean, and results in robust standard errors.

---

## Model agnostic standard errors 

```{r agnostic}
library(sandwich)
confint.agnostic <- function (object, parm, level = 0.95, ...)
{
    cf <- coef(object); pnames <- names(cf)
    if (missing(parm))
        parm <- pnames
    else if (is.numeric(parm))
        parm <- pnames[parm]
    a <- (1 - level)/2; a <- c(a, 1 - a)
    pct <- stats:::format.perc(a, 3)
    fac <- qnorm(a)
    ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                               pct))
    ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
    ci[] <- cf[parm] + ses %o% fac
    ci
}
```
[http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval](http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval)

---

## Estimating confidence intervals

```{r}
confint(glm1)
confint.agnostic(glm1)
```


---

## Rates 

In this example, we have a variable which is the overall number of web hits for two sites (jtleek.com and simplystatistics.org) and we have another variable which is the number of web hits for only simplystatistics.org.

We want to know the number of web hits that simply statistics receives as a percentage of overall web hits.  
- So NHSS is the number of web hits for simplystatistics.org
- And NH is the number of web hits for both.

<br><br>


$$ E[NHSS_i | JD_i, b_0, b_1]/NH_i = \exp\left(b_0 + b_1 JD_i\right) $$

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) - \log(NH_i)  =  b_0 + b_1 JD_i $$

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) = \log(NH_i) + b_0 + b_1 JD_i $$

This shows how we can model our Log Counts (NHSS) by moving the reference (NH) into the RHS as a log offset (a term without a coefficient). This is where we would put in a monitoring time or any reference that we want to measure a variable relative to.

---

## Fitting rates in R 

- In R, the offset can be stated explicitly,
    - `offset=log(visits+1)` 
- or it can simply be added to the model's regressor,
    - ...`~ julian(gaData$date) + log(visits+1)`

```{r ratesFit,dependson="agnostic", cache=TRUE,fig.height=4,fig.width=4}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)
```
Recall that we're adding 1 to the visits to avoid $log(0)$.

---

## Fitting rates in R
Here we fit the rate instead of the count. The model is the same, using a logged offset. The plot just divides by the reference (`visits+1`) (without logging it.)

```{r,dependson="ratesFit",fig.height=4,fig.width=4}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date",
     ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)
```

---

## More information

* [Log-linear models and multiway tables](http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html)
* [Wikipedia on Poisson regression](http://en.wikipedia.org/wiki/Poisson_regression), [Wikipedia on overdispersion](http://en.wikipedia.org/wiki/Overdispersion)
* [Regression models for count data in R](http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf)
* [pscl package](http://cran.r-project.org/web/packages/pscl/index.html) - the function _zeroinfl_ fits zero inflated models. 

We often encounter data with many more zero counts than the Poisson model would anticipate. We call this situation ** zero inflation**. The `pscl` package contains zero inflation models (ZIP) for dealing with exactly that problem.  There are also many other ways of handling it.